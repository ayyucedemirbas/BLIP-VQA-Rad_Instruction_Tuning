{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 11372488,
          "sourceType": "datasetVersion",
          "datasetId": 7119467
        }
      ],
      "dockerImageVersionId": 30919,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ayyucedemirbas/BLIP-VQA-Rad_Instruction_Tuning/blob/main/blip_vqa_rad_instruction_tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q datasets accelerate timm"
      ],
      "metadata": {
        "id": "TEC5LMZ9Nzdg",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-13T10:57:59.31595Z",
          "iopub.execute_input": "2025-04-13T10:57:59.316384Z",
          "iopub.status.idle": "2025-04-13T10:58:03.816512Z",
          "shell.execute_reply.started": "2025-04-13T10:57:59.316341Z",
          "shell.execute_reply": "2025-04-13T10:58:03.815612Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    BlipProcessor,\n",
        "    BlipForConditionalGeneration,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    default_data_collator\n",
        ")\n",
        "import gc"
      ],
      "metadata": {
        "id": "QdUhiUPLN47G",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-13T10:58:03.817764Z",
          "iopub.execute_input": "2025-04-13T10:58:03.818016Z",
          "iopub.status.idle": "2025-04-13T10:58:28.209003Z",
          "shell.execute_reply.started": "2025-04-13T10:58:03.817992Z",
          "shell.execute_reply": "2025-04-13T10:58:28.208082Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login --token token --add-to-git-credential"
      ],
      "metadata": {
        "id": "UGwxEBoSOEl6",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-13T10:58:28.210279Z",
          "iopub.execute_input": "2025-04-13T10:58:28.210919Z",
          "iopub.status.idle": "2025-04-13T10:58:28.958428Z",
          "shell.execute_reply.started": "2025-04-13T10:58:28.210896Z",
          "shell.execute_reply": "2025-04-13T10:58:28.957446Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    BlipProcessor,\n",
        "    BlipForConditionalGeneration,\n",
        "    TrainingArguments,\n",
        "    Trainer\n",
        ")\n",
        "\n",
        "import gc\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from PIL import Image\n",
        "from datasets import load_dataset\n",
        "from sklearn.model_selection import KFold"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-13T10:58:28.959911Z",
          "iopub.execute_input": "2025-04-13T10:58:28.960207Z",
          "iopub.status.idle": "2025-04-13T10:58:28.980229Z",
          "shell.execute_reply.started": "2025-04-13T10:58:28.960169Z",
          "shell.execute_reply": "2025-04-13T10:58:28.979674Z"
        },
        "id": "lv1ythAx2Joo"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"ayyuce/vqa-rad-instructions\")\n",
        "full_dataset = dataset[\"train\"]\n",
        "test_dataset = dataset[\"test\"]"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-13T10:58:28.980978Z",
          "iopub.execute_input": "2025-04-13T10:58:28.981189Z",
          "iopub.status.idle": "2025-04-13T10:58:32.970411Z",
          "shell.execute_reply.started": "2025-04-13T10:58:28.98117Z",
          "shell.execute_reply": "2025-04-13T10:58:32.969709Z"
        },
        "id": "xoT37--92Joo",
        "outputId": "12bdd570-bbb2-4808-ca73-1b07238b4224",
        "colab": {
          "referenced_widgets": [
            "2667eaea05bb473194fd8490d57b9f41",
            "fd2b58837e714967b31b56f955a0a50d",
            "7c45a1b6cb7e4a26a633a024c95b91c2",
            "450a0f687a3040a59ab3a944234df543",
            "94018fd4626c40849b4bf31def275ea3"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "README.md:   0%|          | 0.00/100 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2667eaea05bb473194fd8490d57b9f41"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "train-00000-of-00001.parquet:   0%|          | 0.00/42.3M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fd2b58837e714967b31b56f955a0a50d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "test-00000-of-00001.parquet:   0%|          | 0.00/12.8M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7c45a1b6cb7e4a26a633a024c95b91c2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Generating train split:   0%|          | 0/1793 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "450a0f687a3040a59ab3a944234df543"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Generating test split:   0%|          | 0/451 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "94018fd4626c40849b4bf31def275ea3"
            }
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "num_folds = 5\n",
        "kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
        "indices = np.arange(len(full_dataset))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-13T10:58:32.97128Z",
          "iopub.execute_input": "2025-04-13T10:58:32.971595Z",
          "iopub.status.idle": "2025-04-13T10:58:32.97584Z",
          "shell.execute_reply.started": "2025-04-13T10:58:32.971545Z",
          "shell.execute_reply": "2025-04-13T10:58:32.974808Z"
        },
        "id": "vAHOG3_82Joo"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "best_models = []\n",
        "val_losses = []"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-13T10:58:32.976648Z",
          "iopub.execute_input": "2025-04-13T10:58:32.976864Z",
          "iopub.status.idle": "2025-04-13T10:58:32.997138Z",
          "shell.execute_reply.started": "2025-04-13T10:58:32.976847Z",
          "shell.execute_reply": "2025-04-13T10:58:32.996212Z"
        },
        "id": "Nd3tSeAr2Jop"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def init_model():\n",
        "    model_name = \"Salesforce/blip-vqa-base\"\n",
        "    processor = BlipProcessor.from_pretrained(model_name)\n",
        "    model = BlipForConditionalGeneration.from_pretrained(model_name)\n",
        "    return processor, model"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-13T10:58:32.99907Z",
          "iopub.execute_input": "2025-04-13T10:58:32.999308Z",
          "iopub.status.idle": "2025-04-13T10:58:33.010967Z",
          "shell.execute_reply.started": "2025-04-13T10:58:32.999288Z",
          "shell.execute_reply": "2025-04-13T10:58:33.010217Z"
        },
        "id": "acM8oDAL2Jop"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "for fold, (train_idx, val_idx) in enumerate(kf.split(indices)):\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"STARTING FOLD {fold+1}/{num_folds}\")\n",
        "    print(f\"{'='*50}\")\n",
        "\n",
        "    # Force clear memory before each fold\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    processor, model = init_model()\n",
        "\n",
        "    train_subset = full_dataset.select(train_idx.tolist())\n",
        "    val_subset = full_dataset.select(val_idx.tolist())\n",
        "\n",
        "    print(f\"Train size: {len(train_subset)}, Validation size: {len(val_subset)}\")\n",
        "\n",
        "    def preprocess_function(examples):\n",
        "        images = examples[\"image\"]\n",
        "        questions = examples[\"instruction\"]\n",
        "        answers = examples[\"response\"]\n",
        "\n",
        "        inputs = processor(\n",
        "            images=images,\n",
        "            text=questions,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=64,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        target_encoding = processor.tokenizer(\n",
        "            answers,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=64,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        labels = target_encoding.input_ids.clone()\n",
        "        labels[labels == processor.tokenizer.pad_token_id] = -100\n",
        "\n",
        "        batch = {\n",
        "            \"pixel_values\": inputs.pixel_values,\n",
        "            \"input_ids\": inputs.input_ids,\n",
        "            \"attention_mask\": inputs.attention_mask,\n",
        "            \"labels\": labels\n",
        "        }\n",
        "\n",
        "        return batch\n",
        "\n",
        "    print(f\"Processing training data for fold {fold+1}...\")\n",
        "    train_processed = train_subset.map(\n",
        "        preprocess_function,\n",
        "        batched=True,\n",
        "        batch_size=4,\n",
        "        num_proc=1,\n",
        "        remove_columns=train_subset.column_names,\n",
        "        load_from_cache_file=False,\n",
        "        desc=f\"Processing train fold {fold+1}\"\n",
        "    )\n",
        "\n",
        "    print(f\"Processing validation data for fold {fold+1}...\")\n",
        "    val_processed = val_subset.map(\n",
        "        preprocess_function,\n",
        "        batched=True,\n",
        "        batch_size=4,\n",
        "        num_proc=1,\n",
        "        remove_columns=val_subset.column_names,\n",
        "        load_from_cache_file=False,\n",
        "        desc=f\"Processing val fold {fold+1}\"\n",
        "    )\n",
        "\n",
        "    fold_output_dir = f\"blip-vqa-rad-fold-{fold+1}\"\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=fold_output_dir,\n",
        "        evaluation_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        num_train_epochs=10,\n",
        "        per_device_train_batch_size=2,\n",
        "        per_device_eval_batch_size=2,\n",
        "        learning_rate=3e-5,\n",
        "        weight_decay=0.01,\n",
        "        logging_steps=50,\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"eval_loss\",\n",
        "        greater_is_better=False,\n",
        "        save_total_limit=1,\n",
        "        push_to_hub=False,\n",
        "        report_to=\"none\",\n",
        "        fp16=True if torch.cuda.is_available() else False,\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_processed,\n",
        "        eval_dataset=val_processed,\n",
        "        data_collator=default_data_collator,\n",
        "    )\n",
        "\n",
        "    print(f\"Training fold {fold+1}...\")\n",
        "    trainer.train()\n",
        "\n",
        "    print(f\"Evaluating fold {fold+1}...\")\n",
        "    eval_results = trainer.evaluate()\n",
        "    val_loss = eval_results[\"eval_loss\"]\n",
        "    val_losses.append(val_loss)\n",
        "    print(f\"Validation loss for fold {fold+1}: {val_loss}\")\n",
        "\n",
        "    best_model_path = f\"{fold_output_dir}/best_model\"\n",
        "    trainer.save_model(best_model_path)\n",
        "    best_models.append(best_model_path)\n",
        "    print(f\"Best model for fold {fold+1} saved to {best_model_path}\")\n",
        "\n",
        "    del train_subset, val_subset, train_processed, val_processed\n",
        "    del trainer, model, processor\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    print(f\"Fold {fold+1} complete!\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-13T10:58:33.012292Z",
          "iopub.execute_input": "2025-04-13T10:58:33.012852Z"
        },
        "id": "_-1f-eLn2Jop",
        "outputId": "73054fa9-64c2-4882-a657-6a10fb11e573",
        "colab": {
          "referenced_widgets": [
            "f3b7e48f32bc445d91b01178751fba9c",
            "0f1bddb17e504ee282e65bdb55c6f8cd",
            "dd7c686232624d6385107b6ac35fc1af",
            "1be525f638994a95bde3136995e5d7f3",
            "8f1f1816a5a947d4a0b5b83c50791a0c",
            "ba8db61931924cf2b61f173f9075a71c",
            "66de45f74c2c48ceb34596a64df41dd1",
            "3f12250e55d7477f9dba9db3958f6054",
            "3dd9baa86d9042d29902ac68993a666d"
          ]
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "\n==================================================\nSTARTING FOLD 1/5\n==================================================\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "preprocessor_config.json:   0%|          | 0.00/445 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f3b7e48f32bc445d91b01178751fba9c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer_config.json:   0%|          | 0.00/592 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0f1bddb17e504ee282e65bdb55c6f8cd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dd7c686232624d6385107b6ac35fc1af"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1be525f638994a95bde3136995e5d7f3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8f1f1816a5a947d4a0b5b83c50791a0c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "config.json:   0%|          | 0.00/4.56k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ba8db61931924cf2b61f173f9075a71c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model.safetensors:   0%|          | 0.00/1.54G [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "66de45f74c2c48ceb34596a64df41dd1"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "Train size: 1434, Validation size: 359\nProcessing training data for fold 1...\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Processing train fold 1:   0%|          | 0/1434 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3f12250e55d7477f9dba9db3958f6054"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "Processing validation data for fold 1...\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Processing val fold 1:   0%|          | 0/359 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3dd9baa86d9042d29902ac68993a666d"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Training fold 1...\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='718' max='7170' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 718/7170 09:16 < 1:23:30, 1.29 it/s, Epoch 1/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>3.482500</td>\n      <td>3.186147</td>\n    </tr>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\\n\" + \"=\"*50)\n",
        "print(\"K-FOLD CROSS-VALIDATION SUMMARY\")\n",
        "print(\"=\"*50)\n",
        "for i, (model_path, val_loss) in enumerate(zip(best_models, val_losses)):\n",
        "    print(f\"Fold {i+1}: Model path: {model_path}, Validation Loss: {val_loss}\")\n",
        "\n",
        "best_fold_idx = np.argmin(val_losses)\n",
        "best_fold_model = best_models[best_fold_idx]\n",
        "print(f\"\\nBest model is from fold {best_fold_idx+1} with validation loss {val_losses[best_fold_idx]}\")\n",
        "print(f\"Best model path: {best_fold_model}\")\n",
        "\n",
        "print(\"\\nProcessing test dataset for final evaluation...\")\n",
        "best_processor = BlipProcessor.from_pretrained(best_fold_model)\n",
        "best_model = BlipForConditionalGeneration.from_pretrained(best_fold_model)\n",
        "\n",
        "def test_preprocess_function(examples):\n",
        "    images = examples[\"image\"]\n",
        "    questions = examples[\"instruction\"]\n",
        "    answers = examples[\"response\"]\n",
        "\n",
        "    inputs = best_processor(\n",
        "        images=images,\n",
        "        text=questions,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=64,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    target_encoding = best_processor.tokenizer(\n",
        "        answers,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=64,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    labels = target_encoding.input_ids.clone()\n",
        "    labels[labels == best_processor.tokenizer.pad_token_id] = -100\n",
        "\n",
        "    batch = {\n",
        "        \"pixel_values\": inputs.pixel_values,\n",
        "        \"input_ids\": inputs.input_ids,\n",
        "        \"attention_mask\": inputs.attention_mask,\n",
        "        \"labels\": labels\n",
        "    }\n",
        "\n",
        "    return batch\n",
        "\n",
        "test_processed = test_dataset.map(\n",
        "    test_preprocess_function,\n",
        "    batched=True,\n",
        "    batch_size=4,\n",
        "    num_proc=1,\n",
        "    remove_columns=test_dataset.column_names,\n",
        "    load_from_cache_file=False,\n",
        "    desc=\"Processing test dataset\"\n",
        ")\n",
        "\n",
        "test_args = TrainingArguments(\n",
        "    output_dir=\"./test_results\",\n",
        "    per_device_eval_batch_size=2,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "test_trainer = Trainer(\n",
        "    model=best_model,\n",
        "    args=test_args,\n",
        "    eval_dataset=test_processed,\n",
        "    data_collator=default_data_collator,\n",
        ")\n",
        "\n",
        "print(\"Evaluating best model on test set...\")\n",
        "test_results = test_trainer.evaluate()\n",
        "print(f\"Test Loss: {test_results['eval_loss']}\")\n",
        "\n",
        "def test_model(image_path, question, model_path=None):\n",
        "    \"\"\"\n",
        "    Test a BLIP VQA model with a new image and question.\n",
        "\n",
        "    Args:\n",
        "        image_path: Path to the image file\n",
        "        question: Question text\n",
        "        model_path: Path to the model to use (defaults to best model)\n",
        "\n",
        "    Returns:\n",
        "        The model's answer\n",
        "    \"\"\"\n",
        "    if model_path is None:\n",
        "        model_path = best_fold_model\n",
        "\n",
        "    processor = BlipProcessor.from_pretrained(model_path)\n",
        "    model = BlipForConditionalGeneration.from_pretrained(model_path)\n",
        "\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    image = Image.open(image_path)\n",
        "    inputs = processor(images=image, text=question, return_tensors=\"pt\")\n",
        "\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_length=64,\n",
        "            num_beams=5,\n",
        "            min_length=1,\n",
        "            do_sample=False,\n",
        "            repetition_penalty=1.5\n",
        "        )\n",
        "\n",
        "    answer = processor.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    return answer\n",
        "\n",
        "def ensemble_predict(image_path, question):\n",
        "    \"\"\"\n",
        "    Get predictions from all models in the ensemble\n",
        "\n",
        "    Args:\n",
        "        image_path: Path to the image file\n",
        "        question: Question text\n",
        "\n",
        "    Returns:\n",
        "        List of predictions from all models\n",
        "    \"\"\"\n",
        "    predictions = []\n",
        "\n",
        "    for i, model_path in enumerate(best_models):\n",
        "        print(f\"Getting prediction from fold {i+1} model...\")\n",
        "        pred = test_model(image_path, question, model_path)\n",
        "        predictions.append(pred)\n",
        "        print(f\"Fold {i+1} prediction: {pred}\")\n",
        "\n",
        "    return predictions\n",
        "\n",
        "print(\"\\nTraining and evaluation complete!\")\n",
        "print(\"You can now use test_model() or ensemble_predict() to get predictions for new images.\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "-cOUDaEC2Jop"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}